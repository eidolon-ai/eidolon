{
  "additionalProperties": false,
  "properties": {
    "implementation": {
      "const": "OllamaLLMUnit",
      "title": "Implementation"
    },
    "model": {
      "$ref": "../LLMModel/overview.json"
    },
    "temperature": {
      "default": 0.3,
      "title": "Temperature",
      "type": "number"
    },
    "force_json": {
      "default": true,
      "title": "Force Json",
      "type": "boolean"
    },
    "max_tokens": {
      "anyOf": [
        {
          "type": "integer"
        },
        {
          "type": "null"
        }
      ],
      "default": null,
      "title": "Max Tokens"
    },
    "ollama_host": {
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "null"
        }
      ],
      "description": "Running Ollama location.\nDefaults to envar OLLAMA_HOST with fallback to 127.0.0.1:11434 if that is not set.",
      "title": "Ollama Host"
    },
    "client_options": {
      "default": {},
      "description": "Extra key-value arguments when instantiating ollama.AsyncClient.",
      "title": "Client Options",
      "type": "object"
    }
  },
  "reference_details": {
    "clz": "eidolon_ai_sdk.apu.llm.ollama_llm_unit.OllamaLLMUnit",
    "group": "LLMUnit",
    "name": "OllamaLLMUnit",
    "overrides": {}
  },
  "required": [
    "ollama_host",
    "implementation"
  ],
  "title": "OllamaLLMUnit",
  "type": "object"
}